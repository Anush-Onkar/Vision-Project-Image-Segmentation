{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision_task1_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmA6z7q4Jwya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a22c2029-bc9c-4d55-ed5a-f5222e2fb0b6"
      },
      "source": [
        "#Mountaining the drive for dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-KpO1ReKRs2",
        "outputId": "9335646f-6b86-4b31-9045-9d52dcdd6e70"
      },
      "source": [
        "cd \"/content/drive/MyDrive/NN_Project\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NN_Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcaEuIPDKSVx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e271e85f-4470-4efa-91c5-68b16a05c562"
      },
      "source": [
        "\"\"\"\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "!tar -xvf VOCtrainval_11-May-2012.tar\n",
        "\"\"\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\\n!tar -xvf VOCtrainval_11-May-2012.tar\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQHdX1NpJ4yx",
        "outputId": "f3c2ee45-5cc8-43b3-e4c9-3d92a423c497"
      },
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/de/0c22c6754370ba6b1fa8e53bd6e514d4a41a181125d405a501c215cbdbd6/scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 103kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PRfRIjELwYZ"
      },
      "source": [
        "import os\n",
        "from os.path import join as pjoin\n",
        "import collections\n",
        "import json\n",
        "import torch\n",
        "import imageio\n",
        "import numpy as np\n",
        "import scipy.misc as m\n",
        "import scipy.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn import metrics\n",
        "import torch.nn.functional as fun\n",
        "import sklearn.metrics as metric\n",
        "from matplotlib import colors\n",
        "\n",
        "\n",
        "class pascalVOCDataset(data.Dataset):\n",
        "    \"\"\"Data loader for the Pascal VOC semantic segmentation dataset.\n",
        "\n",
        "    Annotations from both the original VOC data (which consist of RGB images\n",
        "    in which colours map to specific classes) and the SBD (Berkely) dataset\n",
        "    (where annotations are stored as .mat files) are converted into a common\n",
        "    `label_mask` format.  Under this format, each mask is an (M,N) array of\n",
        "    integer values from 0 to 21, where 0 represents the background class.\n",
        "\n",
        "    The label masks are stored in a new folder, called `pre_encoded`, which\n",
        "    is added as a subdirectory of the `SegmentationClass` folder in the\n",
        "    original Pascal VOC data layout.\n",
        "\n",
        "    A total of five data splits are provided for working with the VOC data:\n",
        "        train: The original VOC 2012 training data - 1464 images\n",
        "        val: The original VOC 2012 validation data - 1449 images\n",
        "        trainval: The combination of `train` and `val` - 2913 images\n",
        "        train_aug: The unique images present in both the train split and\n",
        "                   training images from SBD: - 8829 images (the unique members\n",
        "                   of the result of combining lists of length 1464 and 8498)\n",
        "        train_aug_val: The original VOC 2012 validation data minus the images\n",
        "                   present in `train_aug` (This is done with the same logic as\n",
        "                   the validation set used in FCN PAMI paper, but with VOC 2012\n",
        "                   rather than VOC 2011) - 904 images\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        sbd_path=None,\n",
        "        split=\"train_aug\",\n",
        "        is_transform=False,\n",
        "        img_size=512,\n",
        "        augmentations=None,\n",
        "        img_norm=True,\n",
        "        test_mode=False,\n",
        "    ):\n",
        "        self.root = root\n",
        "        self.sbd_path = sbd_path\n",
        "        self.split = split\n",
        "        self.is_transform = is_transform\n",
        "        self.augmentations = augmentations\n",
        "        self.img_norm = img_norm\n",
        "        self.test_mode = test_mode\n",
        "        self.n_classes = 21\n",
        "        self.mean = np.array([104.00699, 116.66877, 122.67892])\n",
        "        self.files = collections.defaultdict(list)\n",
        "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
        "\n",
        "        if not self.test_mode:\n",
        "            for split in [\"train\", \"val\", \"trainval\"]:\n",
        "                path = pjoin(self.root, \"ImageSets/Segmentation\", split + \".txt\")\n",
        "                file_list = tuple(open(path, \"r\"))\n",
        "                file_list = [id_.rstrip() for id_ in file_list]\n",
        "                self.files[split] = file_list\n",
        "            self.setup_annotations()\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [\n",
        "                # add more trasnformations as you see fit\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files[self.split])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im_name = self.files[self.split][index]\n",
        "        im_path = pjoin(self.root, \"JPEGImages\", im_name + \".jpg\")\n",
        "        lbl_path = pjoin(self.root, \"SegmentationClass/pre_encoded\", im_name + \".png\")\n",
        "        im = Image.open(im_path)\n",
        "        lbl = Image.open(lbl_path)\n",
        "        if self.augmentations is not None:\n",
        "            im, lbl = self.augmentations(im, lbl)\n",
        "        if self.is_transform:\n",
        "            im, lbl = self.transform(im, lbl)\n",
        "        return im, torch.clamp(lbl, max=20)\n",
        "\n",
        "    def transform(self, img, lbl):\n",
        "        if self.img_size == (\"same\", \"same\"):\n",
        "            pass\n",
        "        else:\n",
        "            img = img.resize((self.img_size[0], self.img_size[1]))  # uint8 with RGB mode\n",
        "            lbl = lbl.resize((self.img_size[0], self.img_size[1]))\n",
        "        img = self.tf(img)\n",
        "        lbl = torch.from_numpy(np.array(lbl)).long()\n",
        "        lbl[lbl == 255] = 0\n",
        "        return img, lbl\n",
        "\n",
        "    def get_pascal_labels(self):\n",
        "        \"\"\"Load the mapping that associates pascal classes with label colors\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray with dimensions (21, 3)\n",
        "        \"\"\"\n",
        "        return np.asarray(\n",
        "            [\n",
        "                [0, 0, 0],\n",
        "                [128, 0, 0],\n",
        "                [0, 128, 0],\n",
        "                [128, 128, 0],\n",
        "                [0, 0, 128],\n",
        "                [128, 0, 128],\n",
        "                [0, 128, 128],\n",
        "                [128, 128, 128],\n",
        "                [64, 0, 0],\n",
        "                [192, 0, 0],\n",
        "                [64, 128, 0],\n",
        "                [192, 128, 0],\n",
        "                [64, 0, 128],\n",
        "                [192, 0, 128],\n",
        "                [64, 128, 128],\n",
        "                [192, 128, 128],\n",
        "                [0, 64, 0],\n",
        "                [128, 64, 0],\n",
        "                [0, 192, 0],\n",
        "                [128, 192, 0],\n",
        "                [0, 64, 128],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def encode_segmap(self, mask):\n",
        "        \"\"\"Encode segmentation label images as pascal classes\n",
        "\n",
        "        Args:\n",
        "            mask (np.ndarray): raw segmentation label image of dimension\n",
        "              (M, N, 3), in which the Pascal classes are encoded as colours.\n",
        "\n",
        "        Returns:\n",
        "            (np.ndarray): class map with dimensions (M,N), where the value at\n",
        "            a given location is the integer denoting the class index.\n",
        "        \"\"\"\n",
        "        mask = mask.astype(int)\n",
        "        label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\n",
        "        for ii, label in enumerate(self.get_pascal_labels()):\n",
        "            label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\n",
        "        label_mask = label_mask.astype(int)\n",
        "        # print(np.unique(label_mask))\n",
        "        return label_mask\n",
        "\n",
        "    def decode_segmap(self, label_mask, plot=False):\n",
        "        \"\"\"Decode segmentation class labels into a color image\n",
        "\n",
        "        Args:\n",
        "            label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
        "              the class label at each spatial location.\n",
        "            plot (bool, optional): whether to show the resulting color image\n",
        "              in a figure.\n",
        "\n",
        "        Returns:\n",
        "            (np.ndarray, optional): the resulting decoded color image.\n",
        "        \"\"\"\n",
        "        label_colours = self.get_pascal_labels()\n",
        "        r = label_mask.copy()\n",
        "        g = label_mask.copy()\n",
        "        b = label_mask.copy()\n",
        "        for ll in range(0, self.n_classes):\n",
        "            r[label_mask == ll] = label_colours[ll, 0]\n",
        "            g[label_mask == ll] = label_colours[ll, 1]\n",
        "            b[label_mask == ll] = label_colours[ll, 2]\n",
        "        rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
        "        rgb[:, :, 0] = r / 255.0\n",
        "        rgb[:, :, 1] = g / 255.0\n",
        "        rgb[:, :, 2] = b / 255.0\n",
        "        if plot:\n",
        "            plt.imshow(rgb)\n",
        "            plt.show()\n",
        "        else:\n",
        "            return rgb\n",
        "\n",
        "    def setup_annotations(self):\n",
        "        \"\"\"Sets up Berkley annotations by adding image indices to the\n",
        "        `train_aug` split and pre-encode all segmentation labels into the\n",
        "        common label_mask format (if this has not already been done). This\n",
        "        function also defines the `train_aug` and `train_aug_val` data splits\n",
        "        according to the description in the class docstring\n",
        "        \"\"\"\n",
        "        sbd_path = self.sbd_path\n",
        "        target_path = pjoin(self.root, \"SegmentationClass/pre_encoded\")\n",
        "        if not os.path.exists(target_path):\n",
        "            os.makedirs(target_path)\n",
        "        train_aug = self.files[\"train\"]\n",
        "\n",
        "        # keep unique elements (stable)\n",
        "        train_aug = [train_aug[i] for i in sorted(np.unique(train_aug, return_index=True)[1])]\n",
        "        self.files[\"train_aug\"] = train_aug\n",
        "        set_diff = set(self.files[\"val\"]) - set(train_aug)  # remove overlap\n",
        "        self.files[\"train_aug_val\"] = list(set_diff)\n",
        "\n",
        "        pre_encoded = glob.glob(pjoin(target_path, \"*.png\"))\n",
        "        expected = np.unique(self.files[\"train_aug\"] + self.files[\"val\"]).size\n",
        "\n",
        "        if len(pre_encoded) != expected:\n",
        "            print(\"Pre-encoding segmentation masks...\")\n",
        "\n",
        "            for ii in tqdm(self.files[\"trainval\"]):\n",
        "                fname = ii + \".png\"\n",
        "                lbl_path = pjoin(self.root, \"SegmentationClass\", fname)\n",
        "                lbl = self.encode_segmap(m.imread(lbl_path))\n",
        "                lbl = m.toimage(lbl, high=lbl.max(), low=lbl.min())\n",
        "                m.imsave(pjoin(target_path, fname), lbl)\n",
        "\n",
        "        assert expected == 2913, \"unexpected dataset sizes\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNz4Lo0-LyJ1"
      },
      "source": [
        "#Model Structure\n",
        "#VGG16 pretrained model is used to build the model structre, using torch load model is loaded \n",
        "\n",
        "#class name is segment for our model structure \n",
        "class segment(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(segment,self).__init__()\n",
        "\n",
        "    self.features = torch.hub.load('pytorch/vision:v0.9.0', 'vgg16', pretrained=True).features\n",
        "    #last layer of VGG-16 model is modified for image segmentation \n",
        "    self.last_layer= nn.Sequential(nn.Conv2d(512,4096,3),\n",
        "                          nn.ReLU(inplace= True),\n",
        "                          nn.Dropout(p=0.5, inplace=False),\n",
        "                          nn.Conv2d(4096,4096,3),\n",
        "                          nn.ReLU(inplace= True),\n",
        "                          nn.Dropout(p=0.5, inplace=False))\n",
        "    \n",
        "    self.out= nn.Conv2d(4096,num_classes,3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    f1= self.features(x)\n",
        "    f2= self.last_layer(f1)\n",
        "    f3= self.out(f2)\n",
        "\n",
        "    return F.upsample_bilinear(f3, x.size()[2:])\n",
        "    \n",
        "  def classify(self, x):\n",
        "    return self.forward(x).argmax(dim=1)  \n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbVHkRNDL1Bz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e0b85a-788f-4a2a-b6c8-88908ea74bc5"
      },
      "source": [
        "#Check if CUDA is available, if not use the CPU.\n",
        "train_on_GPU = torch.cuda.is_available()\n",
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if train_on_GPU else 'cpu')\n",
        "\n",
        "if CUDA:\n",
        "    print(\"CUDA is available\")\n",
        "    model = segment(21).cuda()\n",
        "    \n",
        "else:\n",
        "    \n",
        "    print(\"CUDA not available, model will be on CPU\")\n",
        "    model = segment(21)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.9.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "segment(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (last_layer): Sequential(\n",
            "    (0): Conv2d(512, 4096, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (out): Conv2d(4096, 21, kernel_size=(3, 3), stride=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ktMYYNHLzjO"
      },
      "source": [
        "#load model \n",
        "#model= segment(21).cuda()\n",
        "#model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onqOwlJEMCXr"
      },
      "source": [
        "local_path = 'VOCdevkit/VOC2012/' # modify it according to your device\n",
        "bs =10   #change later \n",
        "epochs = 20  #change later \n",
        "learning_rate = 0.0001  #change later "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwPwULo1MD0e"
      },
      "source": [
        "import scipy.misc\n",
        "from scipy.misc.pilutil import imread\n",
        "\n",
        "# dataset variable\n",
        "dst = pascalVOCDataset(local_path,is_transform=True)\n",
        "quantity_train = len(dst)\n",
        "\n",
        "# dataloader variable\n",
        "trainloader = torch.utils.data.DataLoader(dst,batch_size=bs,shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1AB4I68MFK3"
      },
      "source": [
        "# loss function- CrossEntropy \n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer variable\n",
        "# using Adam as optimizer \n",
        "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOjNF3cUMGux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9bf3296-3563-4eaa-b677-d90e30d84a6e"
      },
      "source": [
        "#model training \n",
        "dir= \"\"\n",
        "for epoch in range(epochs): \n",
        "    running_loss = 0.0\n",
        "        \n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "\n",
        "    \n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model.forward(inputs)\n",
        "        loss = loss_f(outputs, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 0:    # print every 10 iteration\n",
        "            print('epoch{}, iter{}, loss: {}'.format(epoch,i,loss.data))\n",
        "    #To save model with epoch\n",
        "    #torch.save(model.state_dict(), os.path.join(dir, 'models/epoch-{}.pt'.format(epoch)))\n",
        "    #save whole model         \n",
        "    torch.save(model,os.path.join(dir,'Model.pt' ))  \n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3672: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch0, iter0, loss: 3.152629852294922\n",
            "epoch0, iter10, loss: 3.1488473415374756\n",
            "epoch0, iter20, loss: 3.165856122970581\n",
            "epoch0, iter30, loss: 3.1882784366607666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcO2Bwvf5ovM"
      },
      "source": [
        "#Evaluation metrics calculation\n",
        "def evaluate(ground_truth, predictions):\n",
        "    ground_truth = torch.flatten(ground_truth, start_dim = 0, end_dim = 2)\n",
        "    pred = torch.argmax(predictions, dim=1)\n",
        "    pred = torch.flatten(pred, start_dim = 0, end_dim = 2)\n",
        "    ground_truth = ground_truth.cpu().numpy()\n",
        "    pred = pred.cpu().numpy()\n",
        "\n",
        "    f1_score = metrics.f1_score(ground_truth, pred,average = 'micro')\n",
        "    dice_coeficient = metrics.jaccard_score(ground_truth,pred, average = 'weighted')\n",
        "\n",
        "    return f1_score,dice_coeficient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEz1YDaQMH-U"
      },
      "source": [
        "\"\"\"\n",
        "This will evaluate the metrics for number of epochs provided\n",
        "\"\"\"\n",
        "F1_Score = []\n",
        "Dice_Score = []\n",
        "F1=0\n",
        "dice=0\n",
        "count=0\n",
        "epochs=10\n",
        "\n",
        "for epoch in range(epochs):     \n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        #getting the inputs:data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_f(outputs, labels)\n",
        "        a,b= evaluate(labels,outputs)\n",
        "        F1=F1+a\n",
        "        dice=dice+b\n",
        "        counter=counter+1\n",
        "        if i % 10 == 0:# print every 10 iteration\n",
        "          print('epoch{}, iter{}, loss: {}'.format(epoch,i,loss.data))\n",
        "    #Averaging the F1 score and Dice Score for number of epochs used       \n",
        "    F1_Score.append(F1/count)\n",
        "    Dice_Score.append(dice/count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WIhtgJNoupX"
      },
      "source": [
        "F1,dice= evaluate(labels,outputs)\n",
        "print(\"F1 Score: \" + str(F1))\n",
        "print(\"Dice Score: \" + str(dice))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx7kKFjHMOUs"
      },
      "source": [
        "#Plotting the evaluation metrics\n",
        "\n",
        "titles = ['F1 Score', 'Dice Score']\n",
        "figures, axis = plt.subplots(nrows=1, ncols=2, figsize =(12,4))\n",
        "current = [F1_Score, Dice_Score]\n",
        "e=[1,2,3,4,5,6,7,8,9,10]\n",
        "for epoch in range(epochs): \n",
        "  for i,axs in enumerate(axis.flatten()):\n",
        "      plt.sca(axs)\n",
        "      plt.title(titles[i])\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Score')\n",
        "      plt.plot(e,current[i])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFljht54MPlx"
      },
      "source": [
        "#Ploting the image \n",
        "\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dst, batch_size=1, shuffle=True)\n",
        "#creating iteration of data loader \n",
        "data_loader = iter(data_loader)\n",
        "#Generate colormap object instance \n",
        "color_map = colors.ListedColormap(dst.get_pascal_labels() / 255)\n",
        "\n",
        "Range = list(range(22))\n",
        "#setting seed \n",
        "torch.manual_seed(4444)\n",
        "#generate colormap index to range value \n",
        "norm_instance = colors.BoundaryNorm(Range, color_map.N)\n",
        "for _ in range(10):\n",
        "    image, solution = next(data_loader)\n",
        "     \n",
        "    #subplot for Original Image \n",
        "    Fig = plt.figure(figsize=(10,5))\n",
        "    org = Fig.add_subplot(131)\n",
        "    org.imshow(image[0].transpose(0, 2).transpose(0, 1).numpy())\n",
        "    org.title.set_text(\"Original\")\n",
        "    org.axis(\"off\")\n",
        "\n",
        "    #subplot for Ground Truth Image\n",
        "    grd_truth = Fig.add_subplot(132)\n",
        "    grd_truth.imshow(solution[0].numpy().astype('uint8'), cmap=color_map, norm=norm_instance)\n",
        "    grd_truth.title.set_text(\"GroundTruth\")\n",
        "    grd_truth.axis(\"off\")\n",
        "\n",
        "    #subplot for Output Image\n",
        "    out_img = Fig.add_subplot(133)\n",
        "    with torch.no_grad():\n",
        "        model_output = model.classify(image.to(device)).squeeze().cpu()\n",
        "    out_img.imshow(model_output.numpy().astype('uint8'),  cmap=color_map, norm=norm_instance)\n",
        "    out_img.title.set_text(\"Output Image\")\n",
        "    out_img.axis(\"off\")\n",
        "    plt.show();\n",
        "  \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}